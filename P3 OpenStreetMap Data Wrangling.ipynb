{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom:1px dotted black;\">\n",
    "<font size=\"4\">\n",
    "<font color = D4C36A> **OpenStreetMap Sample Project  \n",
    "Data Wrangling with MongoDB**</font>    \n",
    "_Jason Medina DAND_\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to OpenStreetMap Area for Culver City](https://www.openstreetmap.org/relation/3492137)  \n",
    "[Link to mapzen extracts page](https://mapzen.com/data/metro-extracts/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom:1px dotted black;\">\n",
    "<font size=\"4\">\n",
    "<font color = 'black'>\n",
    "\n",
    "<UL>\n",
    "1) Problems Encountered in the Map  \n",
    "<L1><UL> Audit Street Names  \n",
    "<L1> Inconsistent Postal Codes</UL>  2) Data Overview  \n",
    "3) Additional Ideas  \n",
    " <UL><L1> Contributor statistics and suggestion  \n",
    " <L1> Additional data exploration with MongoDB  \n",
    " <L1> Conclusion</UL>\n",
    "</UL>\n",
    " </font>\n",
    " </div>\n",
    " \n",
    " For this project, the websites below were reviewed to assist with the report: <br>\n",
    " [Sample MongoDB project](https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md)<br>\n",
    " [Fun read, awesome example](https://jameskao.me/everything-you-need-to-know-about-facebooks-flux-graphql-and-relay/)<br>\n",
    " [Excellent examples and code](https://github.com/allanbreyes/udacity-data-science/tree/master/p2)<br>\n",
    " [Good examples and code](https://github.com/paul-reiners/udacity-data-wrangling-mongo-db)<br>\n",
    " \n",
    "<div style=\"border-bottom:1px dotted black;\">\n",
    "<font size=\"4\">\n",
    "<font color = 'black'>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problems encountered in the map \n",
    "\n",
    "#### To improve data quality, we start with a simple data audit like we saw in the OSM case study by matching the last token in the string optionally ending in a period.  The audit.py has a list of unexpected street types and postcodes.  The audit is necessary to prepare for executing data.py to output the json file for MongoDB   \n",
    "+ over abbreviated street names (e.g. \"Blvd.\" and \"Bvd\" or \"ave\" and \"avenue\")\n",
    "+ postal codes starting with CA or not five digits to exclude\n",
    "<div style=\"border-bottom: 1px dotted black;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Street name abbreviations\n",
    "The audit.py file looks for street names not ending in an expected value, then prints a list of street names with updates.\n",
    "\n",
    "   >_South Fairfax Ave => South Fairfax Avenue_\n",
    "\n",
    "   >_north la brea avenue => North La Brea Avenue_\n",
    "\n",
    "   >_W Pico Blvd => W Pico Boulevard_ \n",
    "\n",
    "Applying the title method capitalizes the first word in each street name.  Replacing the cardinal directions (i.e. N,S,E,W) with the full name would improve data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Code formats\n",
    "The zip_code_audit.py file looks for postcode values that are not five digits nor start with '90'.  The zipcodes look clean, however we could improve by data consistency by excluding these records or removing the CA programmatically.\n",
    "\n",
    "> <i> CA 90045 1 <br>\n",
    "> CA 90291 1  \n",
    "> CA 90405 1  \n",
    "> CA 90404 1  \n",
    "> CA 90034 1   \n",
    "> CA 90036 1  \n",
    "> CA 90024 1   \n",
    "> 91108 1   \n",
    "> CA 90232 1</i>\n",
    "<div style=\"border-bottom: 1px dotted black;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Overview \n",
    "\n",
    "Summary statistics about data - see data.py file for complete code:\n",
    "\n",
    ">1) Original xml file size: 603MB    \n",
    "    <i>print(os.path.getsize(os.path.join(path, \"culver-city_ca.osm.json\"))/1024/1024)  \n",
    "\n",
    ">2) New json file size: 903MB  \n",
    "    <i>print(os.path.getsize(filename)/1024/1024)\n",
    "\n",
    ">3) Document count: 3027238  \n",
    "    <i>print(collection.count())\n",
    "\n",
    ">4) Nodes count: 2757124  \n",
    "    <i>print(collection.find({\"type\":\"node\"}).count())\n",
    "\n",
    ">5) Ways count: 270091  \n",
    "    <i>print(collection.find({\"type\":\"way\"}).count())\n",
    "\n",
    ">6) UU count: 569  \n",
    "    <i>print(len(collection.group([\"created.uid\"], {}, {\"count\":0}, \"function(o, p){p.count++}\")))\n",
    "<div style=\"border-bottom: 1px dotted black;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional Ideas\n",
    "\n",
    "With the data in MongoDB, let's look at some other questions and discuss additional ideas.\n",
    " \n",
    "#### 1) User contributions as ratio, top ten Users \n",
    ">```\n",
    "pipeline = [{\"$group\":{\"_id\": \"$created.user\",\n",
    "                       \"count\": {\"$sum\": 1}}},\n",
    "            {\"$project\": {\"ratio\": {\"$divide\" :[\"$count\"\\\n",
    "            ,collection.find().count()]}}},\n",
    "            {\"$sort\": {\"ratio\": -1}},\n",
    "            {\"$limit\": 10}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for doc in result:\n",
    "    pprint.pprint(doc)\n",
    "```\n",
    "<b>User contributions as ratio of total contributions    \n",
    "{u'_id': u'schleuss_imports', u'ratio': 0.1722556997500692}  \n",
    "{u'_id': u'manings_labuildings', u'ratio': 0.12147277485285267}  \n",
    "{u'_id': u'calfarome_labuilding', u'ratio': 0.11513069008779621}  \n",
    "{u'_id': u'ridixcr_import', u'ratio': 0.10187273019167968}  \n",
    "{u'_id': u'kari, totp_labuildings', u'ratio': 0.05909347068185587}  \n",
    "{u'_id': u'kingrollo', u'ratio': 0.05351049372398206}  \n",
    "{u'_id': u'dannykath_labuildings', u'ratio': 0.04586623185887598}  \n",
    "{u'_id': u'Luis36995_labuildings', u'ratio': 0.039377478744651064}  \n",
    "{u'_id': u'RichRico_labuildings', u'ratio': 0.03589146277894239}  \n",
    "{u'_id': u'schleuss', u'ratio': 0.03488097070663093}</b>      \n",
    "\n",
    "> Note first and last indices contain Schleuss.  A quick internet search leads me to suspect this could be data from Jon Schleuss, data viz artist for the LA times.  I could not come up with any ideas for kingrollo or the users ending in _labuildings.\n",
    "\n",
    "#### 2) Top 20 sources\n",
    ">```# top 20 sources:\n",
    "pipeline = [{\"$match\":{\"source\":{\"$exists\":1}}},\n",
    "            {\"$group\":{\"_id\": \"$source\",\n",
    "                       \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\":20}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for doc in result:\n",
    "    pprint.pprint(doc)  \n",
    "```  \n",
    "<br><b> Top 20 sources: <br>\n",
    "{u'_id': u'usgs_imagery', u'count': 541}  \n",
    "{u'_id': u'TIGER, Bing', u'count': 432}  \n",
    "{u'_id': u'Bing', u'count': 427}  \n",
    "{u'_id': u'Yahoo!, local knowledge', u'count': 278}  \n",
    "{u'_id': u'survey;image;usgs_imagery', u'count': 223}  \n",
    "{u'_id': u'Yahoo', u'count': 195}  \n",
    "{u'_id': u'Yahoo,TIGER', u'count': 161}  \n",
    "{u'_id': u'bing_imagery_0.06m_200801', u'count': 124}  \n",
    "{u'_id': u'survey, image, usgs_imagery', u'count': 123}  \n",
    "{u'_id': u'USGS Geonames', u'count': 107}  \n",
    "{u'_id': u'survey', u'count': 95}  \n",
    "{u'_id': u'Bing, local knowledge', u'count': 85}  \n",
    "{u'_id': u'yahoo_imagery', u'count': 72}  \n",
    "{u'_id': u'Yahoo!, Bing, local knowledge', u'count': 62}  \n",
    "{u'_id': u'survey;image;usgs_imagery;CDOT', u'count': 39}  \n",
    "{u'_id': u'bing_imagery_0.06m_200801;LACA', u'count': 31}  \n",
    "{u'_id': u'Bing, TIGER', u'count': 26}  \n",
    "{u'_id': u'usgs_imagery;survey;image', u'count': 17}  \n",
    "{u'_id': u'bing', u'count': 13}  \n",
    "{u'_id': u'Los Angeles Fire Department', u'count': 12}</b>\n",
    "\n",
    ">  Where a source exists, the usgs_imagery appears twice in the top 5, and multiple times overall.  The other sources include Yahoo, Bing and even the LA Fire Department. TIGER stands for Topologically Integrated Geographic Encoding and Referencing, so it makes sense that we see this as a source.\n",
    "\n",
    "#### 3) Amenities near me\n",
    ">```\n",
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"name\":{\"$exists\":1}}},  \n",
    "            {\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}},\n",
    "            {\"$limit\":30}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for doc in result:\n",
    "    pprint.pprint(doc)\n",
    "```<br>\n",
    "\n",
    "><b>Amenities near me:<br>\n",
    "{u'_id': u'restaurant', u'count': 206}  \n",
    "{u'_id': u'school', u'count': 192}  \n",
    "{u'_id': u'place_of_worship', u'count': 181}  \n",
    "{u'_id': u'fast_food', u'count': 120}  \n",
    "{u'_id': u'cafe', u'count': 93}  \n",
    "{u'_id': u'hospital', u'count': 54}  \n",
    "{u'_id': u'parking', u'count': 51}  \n",
    "{u'_id': u'fuel', u'count': 42}  \n",
    "{u'_id': u'library', u'count': 36}  \n",
    "{u'_id': u'bank', u'count': 31}  \n",
    "{u'_id': u'post_office', u'count': 28}  \n",
    "{u'_id': u'pharmacy', u'count': 24}  \n",
    "{u'_id': u'fire_station', u'count': 23}  \n",
    "{u'_id': u'theatre', u'count': 20}  \n",
    "{u'_id': u'bar', u'count': 16}  \n",
    "{u'_id': u'cinema', u'count': 12}  \n",
    "{u'_id': u'police', u'count': 10}\n",
    ".  \n",
    ".  \n",
    ".  \n",
    "{u'_id': u'ice_cream', u'count': 4}  \n",
    "{u'_id': u'clinic', u'count': 4}  \n",
    "{u'_id': u'public_building', u'count': 3}  </b>\n",
    "\n",
    ">  There are 206 restaurants found, note fast food and cafe are also in the top 5.  The restaurants can be further grouped by cuisine type.\n",
    "\n",
    "#### 4) Restaurant amenity grouped by cuisine type\n",
    "```pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1} \\\n",
    "            , \"cuisine\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},        \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":10}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for doc in result:\n",
    "    pprint.pprint(doc)\n",
    "```  \n",
    "<b><br>Restaurant amenties by cusine type:\n",
    "{u'_id': u'mexican', u'count': 28}  \n",
    "{u'_id': u'burger', u'count': 23}  \n",
    "{u'_id': u'coffee_shop', u'count': 22}  \n",
    "{u'_id': u'pizza', u'count': 14}  \n",
    "{u'_id': u'italian', u'count': 13}  \n",
    "{u'_id': u'sandwich', u'count': 12}  \n",
    "{u'_id': u'american', u'count': 12}  \n",
    "{u'_id': u'chinese', u'count': 8}  \n",
    "{u'_id': u'japanese', u'count': 7}  \n",
    "{u'_id': u'sushi', u'count': 7}  </b>\n",
    "\n",
    "> Where the cuisine type exists, for any amenity, we see the most common cuisine is Mexican.  Note Sushi might be too specific, as Sushi is a type of Japanese food; same is true for pizza however I can understand the desire for detail.  The chinese number looks low to me, so there could be an opportunity to add data.\n",
    "<br><br>\n",
    "\n",
    "<div style=\"border-bottom: 1px dotted black;\">\n",
    "</div>\n",
    "\n",
    "#### <font color = grey> Conclusion: <br><br> Other than over abbreviated street names the data looks complete.  This is not unexpected since Culver City is in the major metro area Los Angeles, California.  Interesting to see a columnist from the LA times contributing 20% of the data via two user names. \n",
    "\n",
    "Additional excercises could include reformatting zip codes and cardinal directions though I belive this data is clean enough for this excercise.  Adding some sort of data validation or gamification for human interaction could prove fun.  For example, find amenities such as restaurant, cafe or fast food without cuisine types then ping nearby users to contribute information for points or some sort of recognition award.  There is no uniform format for operating hours, finding a way to validate this data would allow users to identify if places were open or not.  However coming up with methods to standardize formats or obscure personal identifying information may be a challenge.  Nor is it easy to see where or how often updates occur.  \n",
    "\n",
    "Overall this OSM data set is clean and very rich.  I hope to make this an onging project where, with the goal of submitting data to the OSM project.  Stay tuned! </font>   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
